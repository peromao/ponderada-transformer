# ponderada-transformer
 
O transformer me pareceu um modelo que pode performar melhor que RNN ao processar texto. Isso acontece porque o transformer consegue perceber conexões mais distantes entre dados, entretanto, ele demanda mais poder computacional para fazer os seus cálculos. O transformer pode ser paralelizado para treinar mais rápidamente, o que em grande escala pode ser muito bom, mas ao testar no meu computador, levou muito tempo para fazer o treinamento. Para treinar apenas 1 época com os hiperparâmetros: num_layers = 4, d_model = 128, dff = 512 e num_heads = 8; demorou por volta de 1 hora e meia, eram 24 segundos por step. Isso para treinar no colab fica muito demorado, para treinar apenas 20 épocas seriam mais de 1 dia. Nas minhas modificações eu mudei esses hiperparâmetros para:
num_layers = 1, d_model = 16, dff = 32, num_heads = 2; com isso, demorou apenas por volta de 13 minutos por época e 1 segundo por step. Isso custou a acurácia do modelo entretanto para testes com o colab para estudo, fica muito melhor. Executei o código com CPU e GPU porém não tive uma diferença notável no tempo de execução.
